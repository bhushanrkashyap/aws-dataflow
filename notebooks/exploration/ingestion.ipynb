{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e542568-91a8-4a2b-a10c-bbfa6838776e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9886f3fe-7aea-44de-a905-952aca3d0fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"awspipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2157355-031b-4b2b-b957-231aa25e3309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test upload succeeded! Check S3 for 'test_upload.txt'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "TWELVEDATA_API_KEY = 'api_key_here'  \n",
    "aws_access_key_id = 's3_access_key'\n",
    "aws_secret_access_key = 's3_secret_key'\n",
    "region_name = 'region-name'\n",
    "bucket_name = 'your_bucket_name'\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Test S3 connection\n",
    "try:\n",
    "    s3.put_object(Bucket=bucket_name, Key='upload.txt', Body='S3 connection successful!')\n",
    "    print(\"Test upload succeeded! Check S3 for 'test_upload.txt'.\")\n",
    "except Exception as e:\n",
    "    print(\"S3 upload failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f204924d-69c4-43c8-a28d-30a6aa2c73ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Retrieve 250 symbols and their data accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a870127-7ebb-4845-b7a9-61c0cb0557e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_ohlcv_twelvedata(symbol, interval=\"1day\", days=30):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data from TwelveData for the past `days` days.\n",
    "    \"\"\"\n",
    "    url = \"https://api.twelvedata.com/time_series\"\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"outputsize\": days,\n",
    "        \"apikey\": TWELVEDATA_API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"values\" in data:\n",
    "            values = reversed(data[\"values\"])  # Reverse to get oldest first\n",
    "            return [\n",
    "                {\n",
    "                    \"date\": v[\"datetime\"],\n",
    "                    \"open\": float(v[\"open\"]),\n",
    "                    \"high\": float(v[\"high\"]),\n",
    "                    \"low\": float(v[\"low\"]),\n",
    "                    \"close\": float(v[\"close\"]),\n",
    "                    \"volume\": float(v[\"volume\"])\n",
    "                }\n",
    "                for v in values\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"No data for {symbol}: {data.get('message')}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch OHLCV for {symbol}: {response.status_code}\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81383cdf-f04f-4332-a4e7-7f6266d3e84e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL...\n Uploaded AAPL.csv successfully.\nProcessing MSFT...\n Uploaded MSFT.csv successfully.\nProcessing AMZN...\n Uploaded AMZN.csv successfully.\nProcessing GOOGL...\n Uploaded GOOGL.csv successfully.\nProcessing META...\n Uploaded META.csv successfully.\nProcessing TSLA...\n Uploaded TSLA.csv successfully.\nProcessing NVDA...\n Uploaded NVDA.csv successfully.\nAll symbols uploaded individually to S3 as CSV files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "symbols = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \"NVDA\"]\n",
    "\n",
    "\n",
    "for symbol in symbols:\n",
    "    print(f\"Processing {symbol}...\")\n",
    "    ohlcv_data = get_ohlcv_twelvedata(symbol, days=30)\n",
    "\n",
    "    if ohlcv_data:\n",
    "        try:\n",
    "            df = pd.DataFrame(ohlcv_data)\n",
    "            csv_buffer = df.to_csv(index=False)\n",
    "            file_key = f\"ohlcv/{symbol}.csv\"\n",
    "\n",
    "            s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=file_key,\n",
    "                Body=csv_buffer,\n",
    "                ContentType=\"text/csv\"\n",
    "            )\n",
    "            print(f\" Uploaded {symbol}.csv successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to upload {symbol}: {e}\")\n",
    "    time.sleep(7.5)\n",
    "\n",
    "print(\"All symbols uploaded individually to S3 as CSV files!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}